modelname: asymov_machine_translation_seq2seq_transformer
_target_: temos.model.asymov_mt.AsymovMT

traj: ${traj}

#TODO: verify with Attention is All You Need paper
num_encoder_layers: 6
num_decoder_layers: 6
emb_size: 512
nhead: 8
text_vocab_size: ??? # vocab size of text (including '<pad>', '<bos>', '<eos>', '<unk>')
mw_vocab_size: ??? # vocab size of motion words (including '<pad>', '<bos>', '<eos>'; no '<unk>' here)
dim_feedforward: 2048
special_symbols: ${data.special_symbols}

dropout: 0.1

fps: ${data.framerate}
max_frames: ??? # Length of largest mw sequence in train data excluding EOS/BOS

metrics_start_epoch: ${viz_metrics_start_epoch} # epoch numbering starts from 0, epoch >= metrics_start_epoch
metrics_every_n_epoch: ${viz_metrics_every_n_epoch} # (epoch+1)%n metrics_every_n_epoch


decoding_scheme: "diverse"    # "diverse", "beam", "greedy"
beam_width: 5

defaults:
  - transformer: vanilla
  - losses: asymov_mt_temos #since cross entropy loss already implemented there, may need a different class if MT specific losses are added
  - metrics: asymov_recons
  - optim: adamw
  - /model/losses/function/crossEntropy@func_recons
  - /model/losses/function/mse@func_traj

best_ckpt_monitors: ${callback.best_ckpt_monitors}

func_recons:
  ignore_index: ${special_symbol_idx:${data.special_symbols},'<pad>'}